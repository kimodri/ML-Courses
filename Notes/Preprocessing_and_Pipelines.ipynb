{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0888b121-01d9-4d0a-9e36-2ca50567feb5",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d2a9b-25a9-4744-915f-cdb4cc347929",
   "metadata": {},
   "source": [
    "- Impute missing values\n",
    "- Convert categorical data to numeirc values\n",
    "- Scale data\n",
    "- Evaluate multiple supervised learning models simultaneously\n",
    "- Build pipelines\n",
    "\n",
    "We will be using the music dataset through out this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd599566-10da-479f-8cf3-b5674d0abe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36506</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.726</td>\n",
       "      <td>214547.0</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>-14.824</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>92.934</td>\n",
       "      <td>0.618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37591</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.635</td>\n",
       "      <td>190448.0</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>-4.795</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>110.012</td>\n",
       "      <td>0.637</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37658</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.352</td>\n",
       "      <td>456320.0</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>-3.634</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>122.897</td>\n",
       "      <td>0.228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36060</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.488</td>\n",
       "      <td>352280.0</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>-12.020</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>106.063</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35710</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>273693.0</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>-7.787</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>143.995</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  popularity  acousticness  danceability  duration_ms  energy  \\\n",
       "0       36506        60.0      0.896000         0.726     214547.0   0.177   \n",
       "1       37591        63.0      0.003840         0.635     190448.0   0.908   \n",
       "2       37658        59.0      0.000075         0.352     456320.0   0.956   \n",
       "3       36060        54.0      0.945000         0.488     352280.0   0.326   \n",
       "4       35710        55.0      0.245000         0.667     273693.0   0.647   \n",
       "\n",
       "   instrumentalness  liveness  loudness  speechiness    tempo  valence  genre  \n",
       "0          0.000002    0.1160   -14.824       0.0353   92.934    0.618      1  \n",
       "1          0.083400    0.2390    -4.795       0.0563  110.012    0.637      1  \n",
       "2          0.020300    0.1250    -3.634       0.1490  122.897    0.228      1  \n",
       "3          0.015700    0.1190   -12.020       0.0328  106.063    0.323      1  \n",
       "4          0.000297    0.0633    -7.787       0.0487  143.995    0.300      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "music_df = pd.read_csv(\"music.csv\")\n",
    "music_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474b7d4-024e-45e0-8100-b9367841c852",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7efdd-3d98-4a14-a06d-386424fc2e19",
   "metadata": {},
   "source": [
    "When dealing with categorical features, we convert them to binary features called dummy variables (0 and 1) for example:\n",
    "\n",
    "if we have a feature called `genre` with values: Alternative, Anime, Blues, Classical, Country, Electronic, Hiphop, Jazz, Rap, and Rock. We can create dummy features like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5093f9-6ffe-451c-a8a3-90de4961ea5d",
   "metadata": {},
   "source": [
    "| Alternative | Anime | Blues | Classical | Country | Electronic | Hip-Hop | Jazz | Rap | Rock |\r\n",
    "|-------------|-------|-------|-----------|---------|------------|---------|------|-----|------|\r\n",
    "| 1           | 0     | 0     | 0         | 0       | 0          | 0       | 0    | 0   | 0    |\r\n",
    "| 0           | 1     | 0     | 0         | 0       | 0          | 0       | 0    | 0   | 0    |\r\n",
    "| 0           | 0     | 1     | 0         | 0       | 0          | 0       | 0    | 0   | 0    |\r\n",
    "| 0           | 0     | 0     | 1         | 0       | 0          | 0       | 0    | 0   | 0    |\r\n",
    "| 0           | 0     | 0     | 0         | 1       | 0          | 0       | 0    | 0   | 0    |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 1          | 0       | 0    | 0   | 0    |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 1       | 0    | 0   | 0    |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 0       | 1    | 0   | 0    |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 0       | 0    | 1   | 0    |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 0       | 0    | 0   | 1    |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3298fb-a3c4-44c8-a00c-8e98a57e81d7",
   "metadata": {},
   "source": [
    "Now, we may notice that the `Rock` feature maybe redundant because having 0 from `Alternative` to `Rap` so we omit it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0967a-cb82-4a6f-a643-907b65632d93",
   "metadata": {},
   "source": [
    "| Alternative | Anime | Blues | Classical | Country | Electronic | Hip-Hop | Jazz | Rap |\r\n",
    "|-------------|-------|-------|-----------|---------|------------|---------|------|-----|\r\n",
    "| 1           | 0     | 0     | 0         | 0       | 0          | 0       | 0    | 0   |\r\n",
    "| 0           | 1     | 0     | 0         | 0       | 0          | 0       | 0    | 0   |\r\n",
    "| 0           | 0     | 1     | 0         | 0       | 0          | 0       | 0    | 0   |\r\n",
    "| 0           | 0     | 0     | 1         | 0       | 0          | 0       | 0    | 0   |\r\n",
    "| 0           | 0     | 0     | 0         | 1       | 0          | 0       | 0    | 0   |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 1          | 0       | 0    | 0   |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 1       | 0    | 0   |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 0       | 1    | 0   |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 0       | 0    | 1   |\r\n",
    "| 0           | 0     | 0     | 0         | 0       | 0          | 0       | 0    | 0   |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3e5e1-223a-40f0-837c-1ec8ebd727d3",
   "metadata": {},
   "source": [
    "This approach is called One-Hot Encoding (OHE), and omitting one category (Rock in this case) avoids multicollinearity, as the omitted category can be inferred from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef5985-d282-4053-9d08-1340ef92fef6",
   "metadata": {},
   "source": [
    "To be able to do One-Hot-Encoding, here are the tools you can use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee7465-f29b-4803-840c-4c53ca5fe4a6",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding with Pandas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdc4b0-40e7-43d6-8e9c-8882cf197583",
   "metadata": {},
   "source": [
    "You can pass the entire dataframe, if you only hvae one column of categories. But if you want to be specific pass the series of the category column. Or you can also pass the dataframe and the specific columns as seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679c092-560b-4020-ae92-444efb3ed51f",
   "metadata": {},
   "source": [
    "**Functions (pandas)**  \n",
    "\n",
    "| Function               | Description                                        | Syntax                           |\n",
    "|------------------------|----------------------------------------------------|----------------------------------|\n",
    "| `get_dummies`         | Converts categorical variables into dummy/indicator variables | `pd.get_dummies(df, columns=[col])` |\n",
    "\n",
    "**Arguments (pandas)**  \n",
    "\n",
    "| Argument              | Description                                        | Syntax                           |\n",
    "|----------------------|--------------------------------------------------|---------------------------------|\n",
    "| `df`                | The DataFrame containing categorical columns      | `pd.get_dummies(df, columns=[col])` |\n",
    "| `columns`           | Specifies which columns to encode                 | `pd.get_dummies(df, columns=[col])` |\n",
    "| `drop_first`        | Drops the first category to avoid multicollinearity | `pd.get_dummies(df, drop_first=True)` |\n",
    "| `dtype`             | Specifies data type for the output                 | `pd.get_dummies(df, dtype=int)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd109654-f0a3-446e-aab4-64997134e82f",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding with Sklearn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466a0bb-c4a8-48e6-ae92-3f624f9befa9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Functions (scikit-learn)**  \n",
    "\n",
    "| Function               | Description                                         | Syntax                                      |\n",
    "|------------------------|-----------------------------------------------------|---------------------------------------------|\n",
    "| `OneHotEncoder`       | Encodes categorical features as a one-hot numeric array | `OneHotEncoder()`                           |\n",
    "| `fit`                 | Learns the categories from the data                 | `encoder.fit(X)`                            |\n",
    "| `transform`           | Transforms categorical data into one-hot encoded format | `encoder.transform(X)`                      |\n",
    "| `fit_transform`       | Combines `fit` and `transform`                      | `encoder.fit_transform(X)`                  |\n",
    "\n",
    "**Arguments (scikit-learn)**  \n",
    "\n",
    "| Argument              | Description                                        | Syntax                                      |\n",
    "|----------------------|--------------------------------------------------|---------------------------------------------|\n",
    "| `categories`        | Specifies categories manually or auto-detects them | `OneHotEncoder(categories='auto')`         |\n",
    "| `sparse`           | Returns a sparse matrix if `True`, dense array if `False` | `OneHotEncoder(sparse=False)`              |\n",
    "| `handle_unknown`   | Determines how to handle unknown categories         | `OneHotEncoder(handle_unknown='ignore')`   |\n",
    "| `drop`            | Specifies which category to drop to avoid redundancy | `OneHotEncoder(drop='first')`              |\n",
    "| `dtype`           | Specifies data type for the encoded output           | `OneHotEncoder(dtype=int)`                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969ff05-1edb-4779-9807-343ea66efb5f",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556a1d3-e4d4-4d33-9acb-0430eb48715e",
   "metadata": {},
   "source": [
    "### Dropping\n",
    "When we find missing data, a common approach is **dropping** null values amounting to less than 5% of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0991e3-cd6a-46dc-9121-4341734152df",
   "metadata": {},
   "source": [
    "DataFrame.\n",
    "\n",
    "**Functions**\n",
    "\n",
    "| Function          | Description                                                   | Syntax                                      |\n",
    "|-------------------|---------------------------------------------------------------|---------------------------------------------|\n",
    "| dropna            | Removes missing values from the DataFrame                     | `df.dropna(axis=, how=, thresh=, subset=, inplace=)` |\n",
    "\n",
    "**Arguments**\n",
    "\n",
    "| Argument          | Description                                                   | Syntax                                      |\n",
    "|-------------------|---------------------------------------------------------------|---------------------------------------------|\n",
    "| axis              | Determines whether to drop rows or columns                    | `df.dropna(axis=0)` (rows) or `df.dropna(axis=1)` (columns) |\n",
    "| how               | Specifies the condition for dropping                          | `df.dropna(how='any')` (default) or `df.dropna(how='all')` |\n",
    "| thresh            | Requires a minimum number of non-null values to retain        | `df.dropna(thresh=n)` |\n",
    "| subset            | Specifies which columns to check for null values              | `df.dropna(subset=['col1', 'col2'])` |\n",
    "| inplace           | Whether to modify the DataFrame in place                      | `df.dropna(inplace=True)` |\n",
    "\n",
    "**Examples**\n",
    "\n",
    "1. **Drop Rows with Any Null Values**:\n",
    "   ```python\n",
    "   df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9cfd9-da02-4bd0-aae5-e8e4a72540d6",
   "metadata": {},
   "source": [
    "### Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef156fd-93f1-43a8-93e7-734782ce5dfb",
   "metadata": {},
   "source": [
    "Imputation: use subject-matter expertise to replace missing data with educated guesses, it is common to use the **mean**. We also use the **median**\n",
    "- For categorical, we use the mode\n",
    "- Note we must split our data before imputing to avoid leaking test set information to our model, a concept known as data leakage.\n",
    "\n",
    "Before imputing missing values, you should perform the train-test split. This ensures that the imputation process only uses information from the training set, preventing data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ca0f3-dc69-4ae8-b583-64bc1688cef2",
   "metadata": {},
   "source": [
    "**Arguments**\r\n",
    "\r\n",
    "| Arguments       | Description                                                   | Possible Value(s) | Syntax                                      |\r\n",
    "|-----------------|---------------------------------------------------------------|-------------------|---------------------------------------------|\r\n",
    "| strategy        | The imputation strategy to use                                | 'mean', 'median', 'most_frequent', 'constant' | `SimpleImputer(strategy='')`                |\r\n",
    "| fill_value      | The value to use for imputation when strategy is 'constant'   | Any constant value | `SimpleImputer(fill_value='')`              |\r\n",
    "| missing_values  | The placeholder for the missing values                        | `np.nan`, None, etc. | `SimpleImputer(missing_values='')`          |\r\n",
    "| add_indicator   | Whether to add a missing indicator column                     | True, False       | `SimpleImputer(add_indicator=)`             |\r\n",
    "| copy            | Whether to copy the input data                                | True, False       | `SimpleImputer(copy=)`                      |\r\n",
    "| verbose         | Whether to print progress messages                            | Integer (0 or 1)  | `SimpleImputer(verbose=)`                   |\r\n",
    "\r\n",
    "**Functions**\r\n",
    "\r\n",
    "| Functions       | Description                                                   | Syntax                                      |\r\n",
    "|-----------------|---------------------------------------------------------------|---------------------------------------------|\r\n",
    "| SimpleImputer   | Impute missing values using a specified strategy              | `SimpleImputer(strategy='', fill_value='')` |\r\n",
    "| fit_transform   | Fit the imputer on the training set and transform the data    | `imputer.fit_transform(X_train)`            |\r\n",
    "| transform       | Transform the test set using the fitted imputer               | `imputer.transform(X_test)`                 |\r\n",
    "| append          | Combine arrays along a specified axis                         | `np.append(arr1, arr2, axis=)`              |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa76bf-bde7-46be-82a7-aa03b6c8dbe9",
   "metadata": {},
   "source": [
    "**Imputation workflow in scikit-learn:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9eae8-368d-4aea-9146-7a43abc95dd8",
   "metadata": {},
   "source": [
    "- Import `SimpleImputer`\n",
    "- Split the data, either because you will use different imputation technique or becuase you will imputate traning and testing data\n",
    "- Combine the data if you divided it because of different imputation technique\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# since we will be using two different techniques to our dataframe with null it is best to divide them\n",
    "X_category = df['category'].values.reshape(-1, 1)\n",
    "X_number = df.drop(['taregt', 'genre'], axis = 1).values\n",
    "y = df['target'].values\n",
    "\n",
    "X_train_category, X_test_category, y_train, y_test = train_test_split(X_category, y, test_size = 0.2, random_sate = 42)\n",
    "X_train_number, X_test_number, y_train, y_test = train_test_split(X_number, y, test_size = 0.2, random_sate = 42)\n",
    "\n",
    "# to impute missing value, we instantiate an imputer\n",
    "imp_cat = SimpleImputer(strategy = \"most_frequent\") # the mode\n",
    "X_train_category = imp_cat.fit_transform(X_train_category)\n",
    "X_test_category = imp_cat.transform(X_test_cat)\n",
    "\n",
    "imp_num = SimpleImputer(strategy = \"mean\") # mean\n",
    "X_train_number = imp_num.fit_transform(X_train_number)\n",
    "X_test_number = imp_num.transform(X_test_number)\n",
    "\n",
    "# combine the data\n",
    "X_train = np.append(X_train_num, X_train_cat, axis = 1)\n",
    "X_test = np.append(X_test_num, X_test_cat, axis =1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defeb34-7942-4477-afb3-d149cdab4ed1",
   "metadata": {},
   "source": [
    "Another example:\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {\n",
    "    'Feature1': [1, 2, None, 4, 5],\n",
    "    'Feature2': [None, 2, 3, 4, 5],\n",
    "    'Target': [1, 0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on the training set and transform the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set using the imputer fitted on the training set\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "print(\"Imputed Training Set:\\n\", X_train_imputed)\n",
    "print(\"Imputed Test Set:\\n\", X_test_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb849b6-f9de-4e4b-a214-1f1a3fc04884",
   "metadata": {},
   "source": [
    "## Pipeline and ColumnTransformer\n",
    "---\n",
    "Each step but the last is a transformer. The complete guide is on another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7072a-ccba-4e41-87e1-06dab1dadbfc",
   "metadata": {},
   "source": [
    "## Scaling and Centering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8984ba-b349-4c92-8a71-550e12d3b161",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
